{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cebc3261-3623-42c8-a709-ff1d3109e02b",
   "metadata": {},
   "source": [
    "# Seeing Images Through the Eyes of Decision Trees*\n",
    "> Copyright Antonio Piemontese 2025\n",
    "![](images_by_CART.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca25c4c-bc2a-4d4f-8b9e-573649330596",
   "metadata": {},
   "source": [
    "In this article, you’ll learn to:\n",
    "\n",
    "- Turn unstructured, raw image data into structured, informative features.\n",
    "- Train a decision tree classifier for image classification based on extracted image features.\n",
    "- Apply the above concepts to the CIFAR-10 dataset for image classification.\n",
    "Introduction\n",
    "\n",
    "It’s no secret that decision tree-based models excel in a wide range of classification and regression tasks, often based on structured, tabular data. However, when used in combination with the right tools, decision trees can also be a powerful predictive tool for unstructured data such as text or images, and even for time series data. \n",
    "\n",
    "This article demonstrates how decision trees can make sense of image data that has been converted into structured, meaningful features. More specifically, we will show how to turn raw, pixel-level image data into higher-level features that describe image properties like color histograms and edge counts. We’ll then leverage this information to perform predictive tasks, like classification, by training decision trees — all with the aid of Python’s scikit-learn library.\n",
    "\n",
    "Think about it: it’ll be like making a decision tree’s behavior more like to how our human eyes work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f9519-20a5-466c-af77-49783e93add9",
   "metadata": {},
   "source": [
    "**The CIFAR-10 dataset** we will use for the tutorial is a collection of low-resolution, 32×32 pixel color images, with each pixel being described by three RGB values that define its color.\n",
    "\n",
    "![](CIFAR_10_dataset.png)\n",
    "\n",
    "This dataset is available [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Although other commonly used models for image classification, like neural networks, can process images as grids of pixels, decision trees are designed to work with structured data; hence, our primary goal is to convert our raw image data into this structured format.\n",
    "\n",
    "We start by loading the dataset, freely available in the TensorFlow library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a438197-6516-45c2-a3df-8bc279752bf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mtf\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cifar10\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    " \n",
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    " \n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)\n",
    " \n",
    "# Optional: show a few samples (see article image above)\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(X_train[i])\n",
    "    ax.set_title(class_names[y_train[i]])\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a75fc-c3a8-4b24-99cb-b5919638e900",
   "metadata": {},
   "source": [
    "Notice that the loaded dataset is already partitioned into training and test sets, and the output labels (10 different classes) are also separated from the input image data. We just need to allocate these elements correctly using Python tuples, as shown above. For clarity, we also store the class names in a Python list.\n",
    "\n",
    "Next, we define the core function in our code. This function, called extract_features(), takes an image as input and extracts the desired image features. In our example, we will extract features associated with two main image properties: color histograms for each of the three RGB channels (red, green, and blue), and a measure of edge strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da72e21c-066e-4c0b-9db3-5bc2f6e0db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import sobel\n",
    " \n",
    "def extract_features(images, bins_per_channel=8):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        # Color histogram for each of the 3 RGB channels\n",
    "        hist_features = []\n",
    "        for c in range(3):\n",
    "            hist, _ = np.histogram(img[:,:,c], bins=bins_per_channel, range=(0, 255))\n",
    "            hist_features.extend(hist)\n",
    "        \n",
    "        # Edge detection on grayscale image\n",
    "        gray_img = rgb2gray(img)\n",
    "        edges = sobel(gray_img)\n",
    "        edge_strength = np.sum(edges > 0.1)\n",
    "        \n",
    "        # Merging features\n",
    "        features.append(hist_features + [edge_strength])\n",
    "    \n",
    "    return np.array(features, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03823285-f9d6-4732-82c7-798387bc53e6",
   "metadata": {},
   "source": [
    "The number of bins for each computed color histogram is set to 8, so that the density of information describing the image color properties remains at a reasonable level. For edge detection, we use two functions from skimage: rgb2gray and sobel, which together help detect edges on grayscale versions of our original image.\n",
    "\n",
    "Both subsets of features are put together, and the process repeats for every image in the dataset.\n",
    "\n",
    "We now call the function twice: once for the training set, and once for the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f494af4c-98e8-4ba4-a1a5-4c98b9a56a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feats = extract_features(X_train)\n",
    "X_test_feats = extract_features(X_test)\n",
    " \n",
    "print(\"Feature vector size:\", X_train_feats.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ef09d-d722-4dfb-8683-296b98c2cc5e",
   "metadata": {},
   "source": [
    "The resulting number of features containing information about RGB channel histograms and detected edges amounts to 25.\n",
    "\n",
    "That was the hard part! Now we are largely ready to train a decision tree-based classifier that takes extracted features instead of raw image data as inputs. If you are already familiar with training scikit-learn models, the whole process is self-explanatory: we just need to make sure we pass the extracted features, rather than the raw images, as the training and evaluation inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d440fbf-f38b-4878-8f45-2c9bd1e27d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=20)\n",
    "dt_model.fit(X_train_feats, y_train)\n",
    "\n",
    "y_pred_dt = dt_model.predict(X_test_feats)\n",
    "\n",
    "print(\"MODEL 1. Decision Tree (Color histograms + Edge count):\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(classification_report(y_test, y_pred_dt, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ae8032-e2b7-4a64-9347-28dbc6b0ca1d",
   "metadata": {},
   "source": [
    "Unfortunately, the decision tree performs rather poorly on the extracted image features. And guess what: this is entirely normal and expected.\n",
    "\n",
    "Reducing a 32×32 color image to just 25 explanatory features is an over-simplification that misses fine-grained cues and deeper details in the image that help discriminate, for instance, a bird from an airplane, or a dog from a cat. Keep in mind that image subsets belonging to the same class (e.g. ‘plane’) also have great intra-class variations in properties like color distribution. But the important take-home message here is to learn the how-to and limitations of image feature extraction for decision tree classifiers; achieving high accuracy is not our main goal in this tutorial!\n",
    "\n",
    "Nonetheless, would things be any better if we trained a more advanced tree-based model, like a random forest classifier? Let’s find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6647015a-53bc-4670-bed6-efeebc4e19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_feats, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test_feats)\n",
    "\n",
    "print(\"MODEL 2. Random Forest (Color histograms + Edge count)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a34c8-ce5a-46bf-8b95-f7aa04efbc31",
   "metadata": {},
   "source": [
    "Slight improvement here, but still far from perfect. Eager for some homework? Try applying what we learned in this article to an even simpler dataset, like MNIST or fashion MNIST, and see how it performs. It only got a pass mark for classifying airplanes, still failing for the other nine classes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
